{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of Machine \n",
      "Learning \n",
      "Types of \n",
      "Machine \n",
      "Learning\n",
      "Types of \n",
      "Machine \n",
      "Learning\n",
      "CAR\n",
      "Types of \n",
      "Learning\n",
      "Supervised (inductive) learning\n",
      "• Given: training data + desired outputs\n",
      "(labels)\n",
      "Unsupervised learning\n",
      "• Given:\n",
      "training\n",
      "data\n",
      "(without\n",
      "desired\n",
      "outputs)\n",
      "Semi-supervised learning\n",
      "• Given: training data + a few desired outputs\n",
      "Types of \n",
      "Machine \n",
      "Learning\n",
      "Reinforcement learning\n",
      "• A reinforcement learning algorithm, or\n",
      "agent,\n",
      "learns\n",
      "by\n",
      "interacting\n",
      "with\n",
      "its\n",
      "environment.\n",
      "• The agent receives rewards by performing\n",
      "correctly\n",
      "and\n",
      "penalties\n",
      "for\n",
      "performing\n",
      "incorrectly.\n",
      "• The agent learns without intervention from\n",
      "a human by maximizing its reward and\n",
      "minimizing its penalty.\n",
      "Supervised \n",
      "Learning\n",
      "We are given input samples (X) and output\n",
      "samples (y) of a function y = f(X).\n",
      "We can represent the entire data set:\n",
      "Data= (X,y); {Standard Notation}\n",
      "We would like to “learn” f, and evaluate it\n",
      "on new data. Types:\n",
      "–Classification: y is factors (class labels).\n",
      "–Regression: y is continuous, e.g. linear\n",
      "regression.\n",
      "Housing Price \n",
      "Prediction\n",
      "X= 800\n",
      "Y= ?\n",
      "More Features\n",
      "High Dimensional \n",
      "Features\n",
      "y= price\n",
      "Unsupervised \n",
      "Leaning\n",
      "Given only samples X of the data\n",
      "(unlabelled\n",
      "data),\n",
      "we\n",
      "compute\n",
      "a\n",
      "function used to draw inferences.\n",
      "–y is factor: Clustering\n",
      "–y is continuous: Matrix factorization,\n",
      "Kalman filtering, unsupervised neural\n",
      "networks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    # Loop through each page\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        text += page.get_text(\"text\")  # Extract raw text\n",
    "\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "pdf_path = './documents/Lecture 02- Types of ML.pdf'\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "print(extracted_text)  # Print the first 1000 characters of the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of Machine Learning Types of Machine Learning Types of Machine Learning CAR Types of Learning Supervised (inductive) learning Given: training data + desired outputs (labels) Unsupervised learning Given: training data (without desired outputs) Semi-supervised learning Given: training data + a few desired outputs Types of Machine Learning Reinforcement learning A reinforcement learning algorithm, or agent, learns by interacting with its environment. The agent receives rewards by performing correctly and penalties for performing incorrectly. The agent learns without intervention from a human by maximizing its reward and minimizing its penalty. Supervised Learning We are given input samples (X) and output samples (y) of a function y = f(X). We can represent the entire data set: Data= (X,y); {Standard Notation} We would like to learn f, and evaluate it on new data. Types: Classification: y is factors (class labels). Regression: y is continuous, e.g. linear regression. Housing Price Pr\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove unwanted characters (e.g., page numbers, headers, footers)\n",
    "    text = re.sub(r'\\n+', ' ', text)  # Replace newlines with spaces\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    text = text.strip()  # Remove leading and trailing spaces\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "cleaned_text = clean_text(extracted_text)\n",
    "print(cleaned_text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Load pre-trained GPT-2 model\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a pre-trained tokenizer (e.g., for GPT-2)\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Set the pad token to be the same as the eos token\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as pad_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.save_pretrained('./gpt2_local')\n",
    "tokenizer.save_pretrained('./gpt2_local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the model and tokenizer from the local directory\n",
    "tokenizer = AutoTokenizer.from_pretrained('./gpt2_local')\n",
    "model = AutoModelForCausalLM.from_pretrained('./gpt2_local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks: 10\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Example large text (your cleaned_text)\n",
    "large_text = cleaned_text  # Replace with your 12,113-word text\n",
    "\n",
    "# Split text into sentences using nltk\n",
    "#nltk.download('punkt')\n",
    "sentences = nltk.tokenize.sent_tokenize(large_text)\n",
    "\n",
    "# Function to chunk text into pieces of max token length\n",
    "def chunk_text(sentences, tokenizer, max_length):\n",
    "    chunks = []  # To store final chunks\n",
    "    current_chunk = []  # Collect sentences for the current chunk\n",
    "    current_length = 0  # Track token count of the current chunk\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokenized_sentence = tokenizer(sentence, truncation=False, return_tensors=\"pt\")\n",
    "        sentence_length = tokenized_sentence['input_ids'].size(1)\n",
    "\n",
    "        # If a sentence exceeds max_length, split it into smaller chunks\n",
    "        while sentence_length > max_length:\n",
    "            # Tokenize and truncate to max_length\n",
    "            tokenized_chunk = tokenizer(sentence, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "            decoded_chunk = tokenizer.decode(tokenized_chunk['input_ids'][0], skip_special_tokens=True)\n",
    "            chunks.append(decoded_chunk.strip())\n",
    "            \n",
    "            # Update the sentence to exclude the processed chunk\n",
    "            sentence = sentence[len(decoded_chunk):].strip()\n",
    "            tokenized_sentence = tokenizer(sentence, truncation=False, return_tensors=\"pt\")\n",
    "            sentence_length = tokenized_sentence['input_ids'].size(1)\n",
    "\n",
    "        # If adding the current sentence exceeds max_length, finalize the current chunk\n",
    "        if current_length + sentence_length > max_length:\n",
    "            if current_chunk:  # Add only if the chunk isn't empty\n",
    "                chunks.append(\" \".join(current_chunk).strip())\n",
    "            current_chunk = [sentence]  # Start a new chunk with the current sentence\n",
    "            current_length = sentence_length  # Reset token count\n",
    "        else:\n",
    "            # Add sentence to the current chunk\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length\n",
    "\n",
    "    # Add the final chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk).strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Create chunks with max_length=512\n",
    "max_length = 30\n",
    "chunks = chunk_text(sentences, tokenizer, max_length)\n",
    "\n",
    "print(f\"Total Chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: 25 words, 30 tokens\n",
      "Chunk 2: 18 words, 30 tokens\n",
      "Chunk 3: 19 words, 24 tokens\n",
      "Chunk 4: 28 words, 30 tokens\n",
      "Chunk 5: 20 words, 30 tokens\n",
      "Chunk 6: 21 words, 30 tokens\n",
      "Chunk 7: 14 words, 25 tokens\n",
      "Chunk 8: 21 words, 30 tokens\n",
      "Chunk 9: 12 words, 17 tokens\n",
      "Chunk 10: 14 words, 25 tokens\n"
     ]
    }
   ],
   "source": [
    "# Print each chunk and its token count\n",
    "for i, chunk in enumerate(chunks):\n",
    "    tokenized = tokenizer(chunk, return_tensors=\"pt\", truncation=False)\n",
    "    print(f\"Chunk {i+1}: {len(chunk.split())} words, {tokenized['input_ids'].size(1)} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Types of Machine Learning Types of Machine Learning Types of Machine Learning CAR Types of Learning Supervised (inductive) learning Given: training data + desired outputs (labels) Unsupervised learning Given: training data (without desired outputs) Semi-supervised learning Given: training data + a few desired outputs Types of Machine Learning Reinforcement learning A reinforcement learning algorithm, or agent, learns by interacting with its environment.',\n",
       " 'The agent receives rewards by performing correctly and penalties for performing incorrectly.',\n",
       " 'The agent learns without intervention from a human by maximizing its reward and minimizing its penalty.',\n",
       " 'Supervised Learning We are given input samples (X) and output samples (y) of a function y = f(X).',\n",
       " 'We can represent the entire data set: Data= (X,y); {Standard Notation} We would like to learn f, and evaluate it on new data.',\n",
       " 'Types: Classification: y is factors (class labels).',\n",
       " 'Regression: y is continuous, e.g.',\n",
       " 'linear regression.',\n",
       " 'Housing Price Prediction X= 800 Y= ?',\n",
       " 'More Features High Dimensional Features y= price Unsupervised Leaning Given only samples X of the data (unlabelled data), we compute a function used to draw inferences.',\n",
       " 'y is factor: Clustering y is continuous: Matrix factorization, Kalman filtering, unsupervised neural networks.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Types of Machine Learning Types of Machine Learning Types of Machine Learning CAR Types of Learning Supervised (inductive) learning Given: training data + desired outputs',\n",
       " '(labels) Unsupervised learning Given: training data (without desired outputs) Semi-supervised learning Given: training data + a few desired',\n",
       " 'outputs Types of Machine Learning Reinforcement learning A reinforcement learning algorithm, or agent, learns by interacting with its environment.',\n",
       " 'The agent receives rewards by performing correctly and penalties for performing incorrectly. The agent learns without intervention from a human by maximizing its reward and minimizing its penalty.',\n",
       " 'We can represent the entire data set: Data= (X,y); {Standard Notation} We would like to learn f, and evaluate it',\n",
       " 'Supervised Learning We are given input samples (X) and output samples (y) of a function y = f(X). on new data.',\n",
       " 'Types: Classification: y is factors (class labels). Regression: y is continuous, e.g. linear regression.',\n",
       " 'More Features High Dimensional Features y= price Unsupervised Leaning Given only samples X of the data (unlabelled data), we compute a',\n",
       " 'Housing Price Prediction X= 800 Y= ? function used to draw inferences.',\n",
       " 'y is factor: Clustering y is continuous: Matrix factorization, Kalman filtering, unsupervised neural networks.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class LectureNotesDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokenized = self.tokenizer(\n",
    "            self.texts[idx], \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            max_length=self.max_length, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': tokenized['input_ids'].squeeze(0),\n",
    "            'attention_mask': tokenized['attention_mask'].squeeze(0),\n",
    "            'labels': tokenized['input_ids'].squeeze(0)  # Labels are the same as input_ids\n",
    "        }\n",
    "\n",
    "\n",
    "dataset = LectureNotesDataset(chunks, tokenizer, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8.659814834594727\n",
      "Loss: 5.747227191925049\n",
      "Loss: 2.448028564453125\n",
      "Loss: 1.3296897411346436\n",
      "Loss: 0.6615678071975708\n",
      "Loss: 0.3584236204624176\n",
      "Loss: 0.3228161036968231\n",
      "Loss: 0.3476089835166931\n",
      "Loss: 0.4093385338783264\n",
      "Loss: 0.29808083176612854\n",
      "Loss: 0.49591994285583496\n",
      "Loss: 0.34673330187797546\n",
      "Loss: 0.40733399987220764\n",
      "Loss: 0.45797669887542725\n",
      "Loss: 0.35465264320373535\n",
      "Loss: 0.48873254656791687\n",
      "Loss: 0.3520383834838867\n",
      "Loss: 0.4194641709327698\n",
      "Loss: 0.4736669361591339\n",
      "Loss: 0.3265528678894043\n",
      "Loss: 0.4130624532699585\n",
      "Loss: 0.296543687582016\n",
      "Loss: 0.35139966011047363\n",
      "Loss: 0.35396167635917664\n",
      "Loss: 0.27065566182136536\n",
      "Loss: 0.3183683454990387\n",
      "Loss: 0.2578621506690979\n",
      "Loss: 0.3091924488544464\n",
      "Loss: 0.2918037176132202\n",
      "Loss: 0.24887317419052124\n",
      "Loss: 0.29214292764663696\n",
      "Loss: 0.24549004435539246\n",
      "Loss: 0.26764562726020813\n",
      "Loss: 0.28636249899864197\n",
      "Loss: 0.22785261273384094\n",
      "Loss: 0.28072589635849\n",
      "Loss: 0.23573854565620422\n",
      "Loss: 0.25528615713119507\n",
      "Loss: 0.26684319972991943\n",
      "Loss: 0.2092631757259369\n",
      "Loss: 0.2552039921283722\n",
      "Loss: 0.2030508667230606\n",
      "Loss: 0.25074559450149536\n",
      "Loss: 0.24838599562644958\n",
      "Loss: 0.19950954616069794\n",
      "Loss: 0.24641960859298706\n",
      "Loss: 0.19772052764892578\n",
      "Loss: 0.23572346568107605\n",
      "Loss: 0.23601578176021576\n",
      "Loss: 0.19203674793243408\n",
      "Loss: 0.22050614655017853\n",
      "Loss: 0.18973085284233093\n",
      "Loss: 0.23152515292167664\n",
      "Loss: 0.2287701666355133\n",
      "Loss: 0.17987555265426636\n",
      "Loss: 0.21805034577846527\n",
      "Loss: 0.17970293760299683\n",
      "Loss: 0.218149796128273\n",
      "Loss: 0.23973794281482697\n",
      "Loss: 0.18128465116024017\n",
      "Loss: 0.2117554396390915\n",
      "Loss: 0.1605806201696396\n",
      "Loss: 0.2058803290128708\n",
      "Loss: 0.21386291086673737\n",
      "Loss: 0.16648100316524506\n",
      "Loss: 0.19437292218208313\n",
      "Loss: 0.16326060891151428\n",
      "Loss: 0.19296810030937195\n",
      "Loss: 0.19321663677692413\n",
      "Loss: 0.16936959326267242\n",
      "Loss: 0.18813678622245789\n",
      "Loss: 0.15428267419338226\n",
      "Loss: 0.1876983493566513\n",
      "Loss: 0.19279298186302185\n",
      "Loss: 0.15456755459308624\n",
      "Loss: 0.1776639074087143\n",
      "Loss: 0.14938075840473175\n",
      "Loss: 0.1802099347114563\n",
      "Loss: 0.18766434490680695\n",
      "Loss: 0.14132821559906006\n",
      "Loss: 0.17216815054416656\n",
      "Loss: 0.13343361020088196\n",
      "Loss: 0.16782335937023163\n",
      "Loss: 0.17154425382614136\n",
      "Loss: 0.12912297248840332\n",
      "Loss: 0.1497977077960968\n",
      "Loss: 0.13043849170207977\n",
      "Loss: 0.15949955582618713\n",
      "Loss: 0.17626646161079407\n",
      "Loss: 0.13626684248447418\n",
      "Loss: 0.14732414484024048\n",
      "Loss: 0.12264662235975266\n",
      "Loss: 0.15496587753295898\n",
      "Loss: 0.15135490894317627\n",
      "Loss: 0.12333944439888\n",
      "Loss: 0.1457398533821106\n",
      "Loss: 0.11440355330705643\n",
      "Loss: 0.1485672891139984\n",
      "Loss: 0.1679690182209015\n",
      "Loss: 0.11182554066181183\n",
      "Loss: 0.13747543096542358\n",
      "Loss: 0.10050782561302185\n",
      "Loss: 0.14897248148918152\n",
      "Loss: 0.1519494205713272\n",
      "Loss: 0.11197637766599655\n",
      "Loss: 0.12180028110742569\n",
      "Loss: 0.09842447936534882\n",
      "Loss: 0.1402989625930786\n",
      "Loss: 0.13133332133293152\n",
      "Loss: 0.10615833848714828\n",
      "Loss: 0.10963375121355057\n",
      "Loss: 0.10221228748559952\n",
      "Loss: 0.13416612148284912\n",
      "Loss: 0.13417233526706696\n",
      "Loss: 0.10257329791784286\n",
      "Loss: 0.1140083447098732\n",
      "Loss: 0.08239522576332092\n",
      "Loss: 0.11712270975112915\n",
      "Loss: 0.12310238182544708\n",
      "Loss: 0.09099540114402771\n",
      "Loss: 0.10986128449440002\n",
      "Loss: 0.08136335760354996\n",
      "Loss: 0.1330234706401825\n",
      "Loss: 0.11101025342941284\n",
      "Loss: 0.0850571021437645\n",
      "Loss: 0.09571155905723572\n",
      "Loss: 0.07403543591499329\n",
      "Loss: 0.11159101873636246\n",
      "Loss: 0.10663872957229614\n",
      "Loss: 0.08256912231445312\n",
      "Loss: 0.0973556712269783\n",
      "Loss: 0.06898026168346405\n",
      "Loss: 0.1026424840092659\n",
      "Loss: 0.11085476726293564\n",
      "Loss: 0.0779535099864006\n",
      "Loss: 0.08513001352548599\n",
      "Loss: 0.07308799773454666\n",
      "Loss: 0.09593987464904785\n",
      "Loss: 0.0968661829829216\n",
      "Loss: 0.07245960086584091\n",
      "Loss: 0.08362188190221786\n",
      "Loss: 0.07373321801424026\n",
      "Loss: 0.08509978652000427\n",
      "Loss: 0.09881684184074402\n",
      "Loss: 0.07060336321592331\n",
      "Loss: 0.06936774402856827\n",
      "Loss: 0.06248435750603676\n",
      "Loss: 0.09614943712949753\n",
      "Loss: 0.08045917004346848\n",
      "Loss: 0.0758916586637497\n",
      "Loss: 0.06653649359941483\n",
      "Loss: 0.05626721307635307\n",
      "Loss: 0.09465910494327545\n",
      "Loss: 0.07935106754302979\n",
      "Loss: 0.05753253400325775\n",
      "Loss: 0.06984985619783401\n",
      "Loss: 0.04751056432723999\n",
      "Loss: 0.07290565967559814\n",
      "Loss: 0.07643675804138184\n",
      "Loss: 0.05745790898799896\n",
      "Loss: 0.050272706896066666\n",
      "Loss: 0.05075683444738388\n",
      "Loss: 0.071338951587677\n",
      "Loss: 0.07486829161643982\n",
      "Loss: 0.04898747801780701\n",
      "Loss: 0.045160576701164246\n",
      "Loss: 0.05799341946840286\n",
      "Loss: 0.07330974191427231\n",
      "Loss: 0.06341621279716492\n",
      "Loss: 0.048448774963617325\n",
      "Loss: 0.04879504442214966\n",
      "Loss: 0.04235835373401642\n",
      "Loss: 0.052484311163425446\n",
      "Loss: 0.06039366126060486\n",
      "Loss: 0.03310511261224747\n",
      "Loss: 0.0540713332593441\n",
      "Loss: 0.03698452189564705\n",
      "Loss: 0.05785885453224182\n",
      "Loss: 0.045061349868774414\n",
      "Loss: 0.028249917551875114\n",
      "Loss: 0.04860515147447586\n",
      "Loss: 0.03416571393609047\n",
      "Loss: 0.04711759462952614\n",
      "Loss: 0.0443233996629715\n",
      "Loss: 0.02941526100039482\n",
      "Loss: 0.040077872574329376\n",
      "Loss: 0.0376427136361599\n",
      "Loss: 0.040796633809804916\n",
      "Loss: 0.04258009418845177\n",
      "Loss: 0.02319195866584778\n",
      "Loss: 0.041299838572740555\n",
      "Loss: 0.028602484613656998\n",
      "Loss: 0.03691760078072548\n",
      "Loss: 0.03140363469719887\n",
      "Loss: 0.020019717514514923\n",
      "Loss: 0.03419012948870659\n",
      "Loss: 0.027280081063508987\n",
      "Loss: 0.040943946689367294\n",
      "Loss: 0.023336198180913925\n",
      "Loss: 0.019274942576885223\n",
      "Loss: 0.026576491072773933\n",
      "Loss: 0.036490533500909805\n",
      "Loss: 0.02979525737464428\n",
      "Loss: 0.025533651933073997\n",
      "Loss: 0.019987069070339203\n",
      "Loss: 0.03873872384428978\n",
      "Loss: 0.030919121578335762\n",
      "Loss: 0.03945758566260338\n",
      "Loss: 0.020521793514490128\n",
      "Loss: 0.015076098963618279\n",
      "Loss: 0.032978419214487076\n",
      "Loss: 0.023313434794545174\n",
      "Loss: 0.02001723274588585\n",
      "Loss: 0.02843356505036354\n",
      "Loss: 0.012784916907548904\n",
      "Loss: 0.019977014511823654\n",
      "Loss: 0.017581062391400337\n",
      "Loss: 0.02474815770983696\n",
      "Loss: 0.022172681987285614\n",
      "Loss: 0.007387722842395306\n",
      "Loss: 0.019780872389674187\n",
      "Loss: 0.023565026000142097\n",
      "Loss: 0.03327230364084244\n",
      "Loss: 0.008861937560141087\n",
      "Loss: 0.0066561345010995865\n",
      "Loss: 0.024266382679343224\n",
      "Loss: 0.027614938095211983\n",
      "Loss: 0.01712236925959587\n",
      "Loss: 0.010164429433643818\n",
      "Loss: 0.00824760552495718\n",
      "Loss: 0.01771416701376438\n",
      "Loss: 0.016927102580666542\n",
      "Loss: 0.013845355249941349\n",
      "Loss: 0.006902082357555628\n",
      "Loss: 0.012584634125232697\n",
      "Loss: 0.013072069734334946\n",
      "Loss: 0.017728885635733604\n",
      "Loss: 0.025136105716228485\n",
      "Loss: 0.014653360471129417\n",
      "Loss: 0.0068291062489151955\n",
      "Loss: 0.025056377053260803\n",
      "Loss: 0.017012793570756912\n",
      "Loss: 0.013211926445364952\n",
      "Loss: 0.016626611351966858\n",
      "Loss: 0.00868560466915369\n",
      "Loss: 0.011270216666162014\n",
      "Loss: 0.015350752510130405\n",
      "Loss: 0.012940707616508007\n",
      "Loss: 0.021500499919056892\n",
      "Loss: 0.006098758429288864\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(50): \n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'], \n",
    "            attention_mask=batch['attention_mask'], \n",
    "            labels=batch['labels']  # Include labels to compute loss\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Supervised Learning We are given input samples (X) of a function y = f(X). We represent the entire data\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Supervised Learning\"\n",
    "input_tokens = tokenizer(input_text, return_tensors='pt')\n",
    "output_tokens = model.generate(input_tokens['input_ids'], max_length=25, pad_token_id=tokenizer.pad_token_id)\n",
    "output_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Response:\", output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
